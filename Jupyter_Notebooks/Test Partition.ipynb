{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_202\"  # Replace with your Java 8 path\n",
    "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"\\\\bin;\" + os.environ[\"PATH\"]\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] =\"--master local[3] pyspark-shell\"\n",
    "os.environ[\"HADOOP_HOME\"] =\"C:\\\\Hadoop\\\\hadoop-3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, date_format, to_date, lit, concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Partition CSV by Date\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the listings dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_detailed = pd.read_csv('raw_data/listings/listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = ['id','scrape_id','last_scraped','source',\n",
    " 'name',\n",
    " 'host_id',\n",
    " 'host_name',\n",
    " 'host_since',\n",
    " 'host_location',\n",
    " 'host_response_time',\n",
    " 'host_response_rate',\n",
    " 'host_acceptance_rate',\n",
    " 'host_is_superhost',\n",
    " 'host_neighbourhood',\n",
    " 'host_listings_count',\n",
    " 'host_total_listings_count',\n",
    " 'neighbourhood',\n",
    " 'neighbourhood_cleansed',\n",
    " 'latitude',\n",
    " 'longitude',\n",
    " 'property_type',\n",
    " 'room_type',\n",
    " 'accommodates',\n",
    " 'bathrooms',\n",
    " 'bathrooms_text',\n",
    " 'bedrooms',\n",
    " 'beds',\n",
    " 'amenities',\n",
    " 'price',\n",
    " 'minimum_nights',\n",
    " 'maximum_nights',\n",
    " 'minimum_minimum_nights',\n",
    " 'maximum_minimum_nights',\n",
    " 'minimum_maximum_nights',\n",
    " 'maximum_maximum_nights',\n",
    " 'minimum_nights_avg_ntm',\n",
    " 'maximum_nights_avg_ntm',\n",
    " 'calendar_updated',\n",
    " 'has_availability',\n",
    " 'availability_30',\n",
    " 'availability_60',\n",
    " 'availability_90',\n",
    " 'availability_365',\n",
    " 'calendar_last_scraped',\n",
    " 'number_of_reviews',\n",
    " 'number_of_reviews_ltm',\n",
    " 'number_of_reviews_l30d',\n",
    " 'first_review',\n",
    " 'last_review',\n",
    " 'review_scores_rating',\n",
    " 'review_scores_accuracy',\n",
    " 'review_scores_cleanliness',\n",
    " 'review_scores_checkin',\n",
    " 'review_scores_communication',\n",
    " 'review_scores_location',\n",
    " 'review_scores_value',\n",
    " 'license',\n",
    " 'instant_bookable',\n",
    " 'calculated_host_listings_count',\n",
    " 'calculated_host_listings_count_entire_homes',\n",
    " 'calculated_host_listings_count_private_rooms',\n",
    " 'calculated_host_listings_count_shared_rooms',\n",
    " 'reviews_per_month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_detailed = df_reviews_detailed[columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_detailed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert host_response_time to string\n",
    "df_reviews_detailed[\"host_response_time\"] = df_reviews_detailed[\"host_response_time\"].astype(str)\n",
    "df_reviews_detailed[\"host_response_rate\"] = pd.to_numeric(df_reviews_detailed[\"host_response_rate\"].str.rstrip('%'),errors=\"coerce\") / 100\n",
    "df_reviews_detailed[\"host_acceptance_rate\"] = pd.to_numeric(df_reviews_detailed[\"host_acceptance_rate\"].str.rstrip('%'),errors=\"coerce\") / 100\n",
    "df_reviews_detailed[\"host_neighbourhood\"] = df_reviews_detailed[\"host_neighbourhood\"].astype(str)\n",
    "df_reviews_detailed[\"price\"] = pd.to_numeric(df_reviews_detailed[\"price\"].str.lstrip('$'),errors=\"coerce\")\n",
    "df_reviews_detailed[\"first_review\"] = pd.to_datetime(df_reviews_detailed[\"first_review\"],format='%Y-%m-%d')\n",
    "df_reviews_detailed[\"last_review\"] = pd.to_datetime(df_reviews_detailed[\"last_review\"],format='%Y-%m-%d')\n",
    "df_reviews_detailed[\"neighbourhood\"] = df_reviews_detailed[\"neighbourhood\"].astype(str)\n",
    "df_reviews_detailed[\"has_availability\"] = df_reviews_detailed[\"has_availability\"].astype(str)\n",
    "df_reviews_detailed[\"bathrooms_text\"] = df_reviews_detailed[\"bathrooms_text\"].astype(str)\n",
    "df_reviews_detailed[\"host_location\"] = df_reviews_detailed[\"host_location\"].astype(str)\n",
    "df_reviews_detailed[\"host_name\"] = df_reviews_detailed[\"host_name\"].astype(str)\n",
    "df_reviews_detailed[\"host_since\"] = df_reviews_detailed[\"host_since\"].astype(str)\n",
    "df_reviews_detailed[\"property_type\"] = df_reviews_detailed[\"property_type\"].astype(str)\n",
    "df_reviews_detailed[\"room_type\"] = df_reviews_detailed[\"room_type\"].astype(str)\n",
    "df_reviews_detailed[\"host_is_superhost\"] = df_reviews_detailed[\"host_is_superhost\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reviews_detailed.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df_reviews_detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define input and output\n",
    "input_file_path = 'raw_data/listings/'\n",
    "output_file_path = 'output_file_data/listings'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Read files and format data\n",
    "\n",
    "spark_df = spark_df.withColumn(\"formatted_last_scraped\", to_date(col(\"last_scraped\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"day\", date_format(col(\"last_scraped\"), \"dd\")) \\\n",
    "       .withColumn(\"month_year\", date_format(col(\"last_scraped\"), \"MM-yyyy\")) \\\n",
    "       .withColumn(\"file_name\", concat(lit(\"listings_\"), date_format(col(\"formatted_last_scraped\"), \"ddMMyyyy\"), lit(\".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df.select(\"month_year\").show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Partition the data\n",
    "# Write to `output_data/mmyyyy/listings_ddmmyyyy.csv`\n",
    "spark_df.write \\\n",
    "    .partitionBy(\"month_year\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_partition_path = f\"{output_file_path}month_year=*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_partition_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(output_partition_path):\n",
    "    print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_df = spark_df.filter(col(\"month_year\").isNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_df.select(['id','scrape_id','month_year','last_scraped']).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "import random\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_writer = SparkSession.builder \\\n",
    "    .appName(\"WriteCSVTest\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random data\n",
    "data = []\n",
    "for i in range(100):  # 100 rows of data\n",
    "    random_str = ''.join(random.choices(string.ascii_letters, k=5))  # Random string of length 5\n",
    "    random_int = random.randint(1, 100)\n",
    "    random_float = random.uniform(1.0, 100.0)\n",
    "    data.append((random_str, random_int, random_float))\n",
    "\n",
    "# Define the schema for the DataFrame\n",
    "columns = [\"name\", \"age\", \"score\"]\n",
    "\n",
    "# Create a PySpark DataFrame\n",
    "df = spark_writer.createDataFrame(data, columns)\n",
    "\n",
    "# Show the DataFrame schema and a few rows to confirm\n",
    "df.printSchema()\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output path (change this to your desired directory)\n",
    "output_path = \"output_file_data/test_output.csv\"\n",
    "\n",
    "# Write the DataFrame to CSV\n",
    "df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(output_path)\n",
    "\n",
    "# Stop the Spark session\n",
    "spark_writer.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reviews Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize SparkSession\n",
    "spark_reviews = SparkSession.builder \\\n",
    "    .appName(\"Partition CSV by Date\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"300\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_reviews = spark_reviews.read.options(delimiter=\",\", header=True).csv('raw_data/reviews/reviews_detailed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_output_file_path = 'output_file_data/reviews'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_reviews_detailed = pd.read_csv('C:/Users/zhiyu/Dropbox/Yuan/Learning/dbt_sqlite/raw_data/reviews/reviews_detailed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd_reviews_detailed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = spark_reviews.createDataFrame(df_pd_reviews_detailed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.select('listing_id').distinct().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 Read files and format data\n",
    "\n",
    "df_reviews = df_reviews.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "       .withColumn(\"day\", date_format(col(\"date\"), \"dd\")) \\\n",
    "       .withColumn(\"month\", date_format(col(\"date\"), \"MM\")) \\\n",
    "       .withColumn(\"year\", date_format(col(\"date\"), \"yyyy\")) \\\n",
    "       .withColumn(\"file_name\", concat(lit(\"reviews_detailed_\"), date_format(col(\"date\"), \"ddMMyyyy\"), lit(\".csv\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.repartition(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Partition the data\n",
    "# Write to `output_data/mmyyyy/listings_ddmmyyyy.csv`\n",
    "df_reviews.write \\\n",
    "    .partitionBy(\"year\",\"month\",'day') \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(reviews_output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
